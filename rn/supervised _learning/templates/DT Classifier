# Decision Tree Classifier Template
# ---------------------------------
# This template covers:
# 1. Reading a CSV dataset
# 2. Handling missing values (dropna, manual imputation, SimpleImputer)
# 3. Encoding categorical features (Label Encoding, One-Hot Encoding)
# 4. Exploratory Data Analysis (EDA)
# 5. Train-test split
# 6. K-Fold Cross Validation
# 7. Leave-One-Out Cross Validation (LOOCV)
# 8. Model evaluation and performance metrics
# 9. Visualization of performance metrics

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split, KFold, cross_val_score, LeaveOneOut
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score,
    f1_score, confusion_matrix, classification_report
)

# -----------------------------
# 1. Load the dataset
# -----------------------------
df = pd.read_csv('your_dataset.csv')  # <-- Replace with your file path

# -----------------------------
# 2. Handling Missing Values
# -----------------------------
# Option A: Drop rows with any missing values
df_dropna = df.dropna()

# Option B: Manual imputation
# Numeric: median; Categorical: mode
df_manual = df.copy()
for col in df_manual.select_dtypes(include=[np.number]).columns:
    df_manual[col].fillna(df_manual[col].median(), inplace=True)
for col in df_manual.select_dtypes(include=['object', 'category']).columns:
    df_manual[col].fillna(df_manual[col].mode()[0], inplace=True)

# Option C: Automated imputation using SimpleImputer
num_cols = df.select_dtypes(include=[np.number]).columns
cat_cols = df.select_dtypes(include=['object', 'category']).columns
num_imputer = SimpleImputer(strategy='mean')
cat_imputer = SimpleImputer(strategy='most_frequent')

df_imputed = df.copy()
df_imputed[num_cols] = num_imputer.fit_transform(df_imputed[num_cols])
df_imputed[cat_cols] = cat_imputer.fit_transform(df_imputed[cat_cols])

# Select the DataFrame to proceed
df = df_imputed.copy()

# -----------------------------
# 3. Encoding Categorical Variables
# -----------------------------
# Label encode target column 'target'
label_enc = LabelEncoder()
df['target'] = label_enc.fit_transform(df['target'])

# One-Hot encode other categorical features
df = pd.get_dummies(df.drop('target', axis=1), drop_first=True).assign(target=df['target'])

# -----------------------------
# 4. Exploratory Data Analysis (EDA)
# -----------------------------
print(df.head())
print(df.info())
print(df.describe())

# Histograms for numeric features
df.hist(bins=20, figsize=(12, 10))
plt.tight_layout(); plt.show()

# Correlation heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(df.corr(), annot=True, fmt='.2f', cmap='coolwarm')
plt.title('Feature Correlation Matrix')
plt.show()

# -----------------------------
# 5. Train-Test Split
# -----------------------------
X = df.drop('target', axis=1)
y = df['target']
# Stratify on target to preserve class distribution
test_size = 0.2  # 20% test set
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=test_size, random_state=42, stratify=y
)

# -----------------------------
# 6. K-Fold Cross Validation
# -----------------------------
k = 5
kf = KFold(n_splits=k, shuffle=True, random_state=42)
model_dt = DecisionTreeClassifier(random_state=42)

# Use accuracy as scoring metric
cv_scores = cross_val_score(model_dt, X_train, y_train, cv=kf, scoring='accuracy')
print(f"K-Fold CV ({k} folds) accuracy scores: {cv_scores}")
print(f"Mean CV accuracy: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")

# -----------------------------
# 7. Leave-One-Out Cross Validation (LOOCV)
# -----------------------------
loo = LeaveOneOut()
loo_scores = cross_val_score(model_dt, X_train, y_train, cv=loo, scoring='accuracy')
print(f"LOOCV accuracy: {loo_scores.mean():.4f}")

# -----------------------------
# 8. Model Training and Evaluation
# -----------------------------
# Train on full training set
model_dt.fit(X_train, y_train)
# Predict on the test set
y_pred = model_dt.predict(X_test)

# Compute performance metrics
acc = accuracy_score(y_test, y_pred)
prec = precision_score(y_test, y_pred, average='weighted')
rec = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')

print("\nTest Set Performance:")
print(f"Accuracy : {acc:.4f}")
print(f"Precision: {prec:.4f}")
print(f"Recall   : {rec:.4f}")
print(f"F1-score : {f1:.4f}")
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# -----------------------------
# 9. Visualization of Performance Metrics
# -----------------------------
# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# CV accuracy bar plot
plt.figure(figsize=(8, 4))
plt.plot(range(1, k+1), cv_scores, marker='o', linestyle='-')
plt.title('K-Fold CV Accuracy Scores')
plt.xlabel('Fold')
plt.ylabel('Accuracy')
plt.ylim([0, 1])
plt.grid(True)
plt.show()

# LOOCV result (single value)
print(f"Overall LOOCV accuracy: {loo_scores.mean():.4f}")
