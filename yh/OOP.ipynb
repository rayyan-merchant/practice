{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20876412",
   "metadata": {},
   "source": [
    "# **CSP:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e288206b",
   "metadata": {},
   "source": [
    "### Basic CSP Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e602fb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ortools.sat.python import cp_model \n",
    "\n",
    "model = cp_model.CpModel()\n",
    "\n",
    "num_vals = 3 \n",
    "x = model.new_int_var(0, num_vals - 1, \"x\")\n",
    "y = model.new_int_var(0, num_vals - 1, \"y\")\n",
    "z = model.new_int_var(0, num_vals - 1, \"z\")\n",
    "\n",
    "model.add(x != y)\n",
    "\n",
    "solver = cp_model.CpSolver()\n",
    "status = solver.solve(model)\n",
    "\n",
    "if status == cp_model.OPTIMAL or status == cp_model.FEASIBLE:\n",
    "    print(f\"x = {solver.value(x)}\")\n",
    "    print(f\"y = {solver.value(y)}\")\n",
    "    print(f\"z = {solver.value(z)}\")\n",
    "else:\n",
    "    print(\"No solution found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5009bae5",
   "metadata": {},
   "source": [
    "### Enumerates all solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28babbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ortools.sat.python import cp_model\n",
    "\n",
    "# Custom solution printer to print all solutions\n",
    "class VarArraySolutionPrinter(cp_model.CpSolverSolutionCallback):\n",
    "    \"\"\"Print intermediate solutions.\"\"\"\n",
    "\n",
    "    def __init__(self, variables: list[cp_model.IntVar]):\n",
    "        cp_model.CpSolverSolutionCallback.__init__(self)\n",
    "        self.__variables = variables\n",
    "        self.__solution_count = 0\n",
    "\n",
    "    def on_solution_callback(self) -> None:\n",
    "        self.__solution_count += 1\n",
    "        for v in self.__variables:\n",
    "            print(f\"{v}={self.value(v)}\", end=\" \")\n",
    "        print()\n",
    "\n",
    "    @property\n",
    "    def solution_count(self) -> int:\n",
    "        return self.__solution_count\n",
    "\n",
    "# Create the model\n",
    "model = cp_model.CpModel()\n",
    "\n",
    "# Define variables (values from 0 to 2)\n",
    "num_vals = 3\n",
    "x = model.new_int_var(0, num_vals - 1, \"x\")\n",
    "y = model.new_int_var(0, num_vals - 1, \"y\")\n",
    "z = model.new_int_var(0, num_vals - 1, \"z\")\n",
    "\n",
    "# Add constraint\n",
    "model.add(x != y)\n",
    "\n",
    "# Set up solver and solution printer\n",
    "solver = cp_model.CpSolver()\n",
    "solution_printer = VarArraySolutionPrinter([x, y, z])\n",
    "\n",
    "# Request enumeration of all solutions\n",
    "solver.parameters.enumerate_all_solutions = True\n",
    "\n",
    "# Solve the model\n",
    "status = solver.solve(model, solution_printer)\n",
    "\n",
    "# Print status and solution count\n",
    "print(f\"Status = {solver.status_name(status)}\")\n",
    "print(f\"Number of solutions found: {solution_printer.solution_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1a9b5b",
   "metadata": {},
   "source": [
    "### Linear Programming problem with constraints and objective maximization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d20b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ortools.sat.python import cp_model\n",
    "\n",
    "def main() -> None:\n",
    "    \"\"\"Minimal CP-SAT example to showcase calling the solver.\"\"\"\n",
    "    # Creates the model.\n",
    "    model = cp_model.CpModel()\n",
    "\n",
    "    # Creates the variables.\n",
    "    var_upper_bound = max(50, 45, 37)\n",
    "    x = model.new_int_var(0, var_upper_bound, \"x\")\n",
    "    y = model.new_int_var(0, var_upper_bound, \"y\")\n",
    "    z = model.new_int_var(0, var_upper_bound, \"z\")\n",
    "\n",
    "    # Creates the constraints.\n",
    "    model.add(2 * x + 7 * y + 3 * z <= 50)\n",
    "    model.add(3 * x - 5 * y + 7 * z <= 45)\n",
    "    model.add(5 * x + 2 * y - 6 * z <= 37)\n",
    "\n",
    "    model.maximize(2 * x + 2 * y + 3 * z)\n",
    "\n",
    "    # Creates a solver and solves the model.\n",
    "    solver = cp_model.CpSolver()\n",
    "    status = solver.solve(model)\n",
    "\n",
    "    if status == cp_model.OPTIMAL or status == cp_model.FEASIBLE:\n",
    "        print(f\"Maximum of objective function: {solver.objective_value}\\n\")\n",
    "        print(f\"x = {solver.value(x)}\")\n",
    "        print(f\"y = {solver.value(y)}\")\n",
    "        print(f\"z = {solver.value(z)}\")\n",
    "    else:\n",
    "        print(\"No solution found.\")\n",
    "\n",
    "    # Statistics.\n",
    "    print(\"\\nStatistics\")\n",
    "    print(f\"  status   : {solver.status_name(status)}\")\n",
    "    print(f\"  conflicts: {solver.num_conflicts}\")\n",
    "    print(f\"  branches : {solver.num_branches}\")\n",
    "    print(f\"  wall time: {solver.wall_time} s\")\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52542dd6",
   "metadata": {},
   "source": [
    "### Job Scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66eb8563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Schedule:\n",
      "Job 1 starts at time 0\n",
      "Job 2 starts at time 5\n",
      "Job 3 starts at time 3\n",
      "Total time (makespan): 7\n"
     ]
    }
   ],
   "source": [
    "from ortools.sat.python import cp_model\n",
    "\n",
    "# Create the model\n",
    "model = cp_model.CpModel()\n",
    "\n",
    "# Job durations and number of jobs\n",
    "job_durations = [3, 2, 2]\n",
    "num_jobs = len(job_durations)\n",
    "horizon = sum(job_durations)  # Maximum total time needed\n",
    "\n",
    "# Create start time variables for each job\n",
    "start_0 = model.new_int_var(0, horizon, \"start_0\")\n",
    "start_1 = model.new_int_var(0, horizon, \"start_1\")\n",
    "start_2 = model.new_int_var(0, horizon, \"start_2\")\n",
    "\n",
    "# Create interval variables (for no-overlap constraint)\n",
    "interval_0 = model.new_interval_var(start_0, job_durations[0], start_0 + job_durations[0], \"interval_0\")\n",
    "interval_1 = model.new_interval_var(start_1, job_durations[1], start_1 + job_durations[1], \"interval_1\")\n",
    "interval_2 = model.new_interval_var(start_2, job_durations[2], start_2 + job_durations[2], \"interval_2\")\n",
    "\n",
    "# Ensure no jobs overlap\n",
    "model.add_no_overlap([interval_0, interval_1, interval_2])\n",
    "\n",
    "# Optional: Minimize makespan (when the last job finishes)\n",
    "makespan = model.new_int_var(0, horizon, \"makespan\")\n",
    "model.add(makespan >= start_0 + job_durations[0])\n",
    "model.add(makespan >= start_1 + job_durations[1])\n",
    "model.add(makespan >= start_2 + job_durations[2])\n",
    "model.minimize(makespan)\n",
    "\n",
    "# Solve the model\n",
    "solver = cp_model.CpSolver()\n",
    "status = solver.solve(model)\n",
    "\n",
    "# Output the result\n",
    "if status == cp_model.OPTIMAL or status == cp_model.FEASIBLE:\n",
    "    print(\"Job Schedule:\")\n",
    "    print(f\"Job 1 starts at time {solver.value(start_0)}\")\n",
    "    print(f\"Job 2 starts at time {solver.value(start_1)}\")\n",
    "    print(f\"Job 3 starts at time {solver.value(start_2)}\")\n",
    "    print(f\"Total time (makespan): {solver.value(makespan)}\")\n",
    "else:\n",
    "    print(\"No solution found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639c9d63",
   "metadata": {},
   "source": [
    "### Map Coloring Problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6353fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ortools.sat.python import cp_model\n",
    "\n",
    "def map_coloring():\n",
    "    model = cp_model.CpModel()\n",
    "\n",
    "    colors = ['Red', 'Green', 'Blue']\n",
    "    color_ids = range(len(colors))\n",
    "\n",
    "    regions = ['WA', 'NT', 'SA', 'Q', 'NSW', 'V', 'T']\n",
    "    variables = {r: model.new_int_var(0, len(colors)-1, r) for r in regions}\n",
    "\n",
    "    # Adjacency constraints\n",
    "    neighbors = [\n",
    "        ('WA', 'NT'), ('WA', 'SA'),\n",
    "        ('NT', 'SA'), ('NT', 'Q'),\n",
    "        ('SA', 'Q'), ('SA', 'NSW'), ('SA', 'V'),\n",
    "        ('Q', 'NSW'), ('NSW', 'V')\n",
    "    ]\n",
    "    for r1, r2 in neighbors:\n",
    "        model.add(variables[r1] != variables[r2])\n",
    "\n",
    "    solver = cp_model.CpSolver()\n",
    "    status = solver.solve(model)\n",
    "\n",
    "    if status in (cp_model.FEASIBLE, cp_model.OPTIMAL):\n",
    "        for r in regions:\n",
    "            print(f\"{r}: {colors[solver.value(variables[r])]}\")\n",
    "    else:\n",
    "        print(\"No solution found.\")\n",
    "\n",
    "map_coloring()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ee292e",
   "metadata": {},
   "source": [
    "### Sudoko Solver:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37b74aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 3, 1, 2]\n",
      "[2, 1, 3, 4]\n",
      "[1, 2, 4, 3]\n",
      "[3, 4, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "from ortools.sat.python import cp_model\n",
    "\n",
    "def sudoku_4x4():\n",
    "    model = cp_model.CpModel()\n",
    "    cells = {}\n",
    "\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            cells[(i,j)] = model.new_int_var(1, 4, f'cell_{i}_{j}')\n",
    "\n",
    "    # Row and column constraints\n",
    "    for i in range(4):\n",
    "        model.add_all_different([cells[(i,j)] for j in range(4)])\n",
    "        model.add_all_different([cells[(j,i)] for j in range(4)])\n",
    "\n",
    "    # 2x2 box constraints\n",
    "    for box_row in range(2):\n",
    "        for box_col in range(2):\n",
    "            box = []\n",
    "            for i in range(2):\n",
    "                for j in range(2):\n",
    "                    box.append(cells[(box_row*2 + i, box_col*2 + j)])\n",
    "            model.add_all_different(box)\n",
    "\n",
    "    # Partial clues\n",
    "    model.add(cells[(0,1)] == 3)\n",
    "    model.add(cells[(1,3)] == 4)\n",
    "    model.add(cells[(2,0)] == 1)\n",
    "    model.add(cells[(3,2)] == 2)\n",
    "\n",
    "    solver = cp_model.CpSolver()\n",
    "    status = solver.solve(model)\n",
    "\n",
    "    if status in (cp_model.FEASIBLE, cp_model.OPTIMAL):\n",
    "        for i in range(4):\n",
    "            print([solver.value(cells[(i,j)]) for j in range(4)])\n",
    "    else:\n",
    "        print(\"No solution found.\")\n",
    "\n",
    "sudoku_4x4()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab126c66",
   "metadata": {},
   "source": [
    "### Send + More = Money (Cryptarithmetic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9fb8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ortools.sat.python import cp_model\n",
    "\n",
    "def send_more_money():\n",
    "    model = cp_model.CpModel()\n",
    "    letters = 'SENDMORY'\n",
    "    vars = {l: model.new_int_var(0, 9, l) for l in letters}\n",
    "\n",
    "    model.add_all_different(vars.values())\n",
    "    model.add(vars['S'] != 0)\n",
    "    model.add(vars['M'] != 0)\n",
    "\n",
    "    send = 1000*vars['S'] + 100*vars['E'] + 10*vars['N'] + vars['D']\n",
    "    more = 1000*vars['M'] + 100*vars['O'] + 10*vars['R'] + vars['E']\n",
    "    money = 10000*vars['M'] + 1000*vars['O'] + 100*vars['N'] + 10*vars['E'] + vars['Y']\n",
    "\n",
    "    model.add(send + more == money)\n",
    "\n",
    "    solver = cp_model.CpSolver()\n",
    "    status = solver.solve(model)\n",
    "\n",
    "    if status in (cp_model.FEASIBLE, cp_model.OPTIMAL):\n",
    "        for l in letters:\n",
    "            print(f\"{l} = {solver.value(vars[l])}\")\n",
    "    else:\n",
    "        print(\"No solution found.\")\n",
    "\n",
    "send_more_money()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eba9f69",
   "metadata": {},
   "source": [
    "### N-Queens Problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e64334",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ortools.sat.python import cp_model\n",
    "\n",
    "def n_queens(n=4):\n",
    "    model = cp_model.CpModel()\n",
    "    queens = [model.new_int_var(0, n-1, f'q{i}') for i in range(n)]\n",
    "\n",
    "    # Row and diagonal constraints\n",
    "    model.add_all_different(queens)  # Columns\n",
    "    model.add_all_different([queens[i] + i for i in range(n)])  # Major diagonal\n",
    "    model.add_all_different([queens[i] - i for i in range(n)])  # Minor diagonal\n",
    "\n",
    "    solver = cp_model.CpSolver()\n",
    "    status = solver.solve(model)\n",
    "\n",
    "    if status in (cp_model.FEASIBLE, cp_model.OPTIMAL):\n",
    "        for row in range(n):\n",
    "            line = ['Q' if solver.value(queens[row]) == col else '.' for col in range(n)]\n",
    "            print(' '.join(line))\n",
    "    else:\n",
    "        print(\"No solution found.\")\n",
    "\n",
    "n_queens()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131ee8b2",
   "metadata": {},
   "source": [
    "# Simple Beam Search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a98d46bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path found: S → C → H → I → L with total cost: 27\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "\n",
    "# Define the graph as an adjacency list with edge costs\n",
    "graph = {\n",
    "    'S': [('A', 3), ('B', 6), ('C', 5)],\n",
    "    'A': [('D', 9), ('E', 8)],\n",
    "    'B': [('F', 12), ('G', 14)],\n",
    "    'C': [('H', 7)],\n",
    "    'H': [('I', 5), ('J', 6)],\n",
    "    'I': [('K', 1), ('L', 10), ('M', 2)],\n",
    "    'D': [], 'E': [], 'F': [], 'G': [], 'J': [],\n",
    "    'K': [], 'L': [], 'M': []  # Leaf nodes\n",
    "}\n",
    "\n",
    "# Beam Search function\n",
    "def beam_search(start, goal, beam_width=2):\n",
    "    # Initialize the beam with the start state\n",
    "    beam = [(0, [start])]  # (cumulative cost, path)\n",
    "\n",
    "    while beam:\n",
    "        candidates = []\n",
    "\n",
    "        # Expand each path in the beam\n",
    "        for cost, path in beam:\n",
    "            current_node = path[-1]\n",
    "            if current_node == goal:\n",
    "                return path, cost  # Return the path and cost if goal is reached\n",
    "\n",
    "            # Generate successors\n",
    "            for neighbor, edge_cost in graph.get(current_node, []):\n",
    "                new_cost = cost + edge_cost\n",
    "                new_path = path + [neighbor]\n",
    "                # print(current_node, new_path)\n",
    "                candidates.append((new_cost, new_path))\n",
    "\n",
    "        # Select top-k paths based on the lowest cumulative cost\n",
    "        beam = heapq.nsmallest(beam_width, candidates, key=lambda x: x[0])\n",
    "        # print(beam)\n",
    "        # print(\"can: \", candidates)\n",
    "    return None, float('inf')  # Return None if no path is found\n",
    "\n",
    "# Run Beam Search\n",
    "start_node = 'S'\n",
    "goal_node = 'L'\n",
    "beam_width = 3\n",
    "path, cost = beam_search(start=start_node, goal=goal_node, beam_width=beam_width)\n",
    "\n",
    "# Print results\n",
    "if path:\n",
    "    print(f\"Path found: {' → '.join(path)} with total cost: {cost}\")\n",
    "else:\n",
    "    print(\"No path found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a95829b",
   "metadata": {},
   "source": [
    "# Hill Climbing:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28b4ec9",
   "metadata": {},
   "source": [
    "### Simple Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "448b4ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best solution found: x = 5, f(x) = 25\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return -x**2 + 10*x  # The function we want to maximize\n",
    "\n",
    "def hill_climb(start, step_size=1, max_iterations=100):\n",
    "    current_x = start\n",
    "    current_value = f(current_x)\n",
    "\n",
    "    for _ in range(max_iterations):\n",
    "        # Check neighbors\n",
    "        next_x1 = current_x + step_size\n",
    "        next_x2 = current_x - step_size\n",
    "\n",
    "        next_value1 = f(next_x1)\n",
    "        next_value2 = f(next_x2)\n",
    "\n",
    "        # Choose the best neighbor\n",
    "        if next_value1 > current_value:\n",
    "            current_x, current_value = next_x1, next_value1\n",
    "        elif next_value2 > current_value:\n",
    "            current_x, current_value = next_x2, next_value2\n",
    "        else:\n",
    "            # No better neighbor found, stop\n",
    "            break\n",
    "\n",
    "    return current_x, current_value\n",
    "\n",
    "# Run hill climbing\n",
    "solution, value = hill_climb(start=0)\n",
    "print(f\"Best solution found: x = {solution}, f(x) = {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e3cb4f",
   "metadata": {},
   "source": [
    "### TSP using Hill-Climbing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "916da4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best route: [(0, 0), (2, 4), (6, 6), (8, 3), (5, 2)]\n",
      "Cost: 21.097265652766033\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "# Distance between cities (coordinates)\n",
    "cities = [(0, 0), (2, 4), (5, 2), (6, 6), (8, 3)]\n",
    "\n",
    "def distance(a, b):\n",
    "    return math.hypot(a[0] - b[0], a[1] - b[1])\n",
    "\n",
    "def total_distance(route):\n",
    "    return sum(distance(route[i], route[(i + 1) % len(route)]) for i in range(len(route)))\n",
    "\n",
    "def get_neighbor(route):\n",
    "    a, b = random.sample(range(len(route)), 2)\n",
    "    neighbor = route[:]\n",
    "    neighbor[a], neighbor[b] = neighbor[b], neighbor[a]\n",
    "    return neighbor\n",
    "\n",
    "def hill_climb_tsp():\n",
    "    current = cities[:]\n",
    "    random.shuffle(current)\n",
    "    current_cost = total_distance(current)\n",
    "\n",
    "    for _ in range(1000):\n",
    "        neighbor = get_neighbor(current)\n",
    "        cost = total_distance(neighbor)\n",
    "        if cost < current_cost:\n",
    "            current, current_cost = neighbor, cost\n",
    "    return current, current_cost\n",
    "\n",
    "best_route, best_cost = hill_climb_tsp()\n",
    "print(f\"Best route: {best_route}\\nCost: {best_cost}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f1cbca",
   "metadata": {},
   "source": [
    "### 8-Queens Problem using simple hill climbing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf8447db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution found for 8-Queens problem:\n",
      "[6, 3, 1, 7, 5, 0, 2, 4]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Heuristic function: Counts the number of pairs of attacking queens\n",
    "def calculate_conflicts(state):\n",
    "\n",
    "    conflicts = 0\n",
    "    n = len(state)\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            # Check same column or diagonal\n",
    "            if state[i] == state[j] or abs(state[i] - state[j]) == abs(i - j):\n",
    "                conflicts += 1\n",
    "    return conflicts\n",
    "\n",
    "# Generate neighbors by moving one queen at a time\n",
    "def get_neighbors(state):\n",
    "    neighbors = []\n",
    "    n = len(state)\n",
    "    for row in range(n):\n",
    "        for col in range(n):\n",
    "            if col != state[row]:\n",
    "                new_state = list(state)\n",
    "                new_state[row] = col\n",
    "                neighbors.append(new_state)\n",
    "    return neighbors\n",
    "\n",
    "# Simple Hill Climbing function\n",
    "def simple_hill_climbing(n):\n",
    "    # Random initial state\n",
    "    current_state = [random.randint(0, n - 1) for _ in range(n)]\n",
    "    current_conflicts = calculate_conflicts(current_state)\n",
    "\n",
    "    while True:\n",
    "        neighbors = get_neighbors(current_state)\n",
    "        next_state = None\n",
    "        next_conflicts = current_conflicts\n",
    "\n",
    "        # Find the first better neighbor\n",
    "        for neighbor in neighbors:\n",
    "            neighbor_conflicts = calculate_conflicts(neighbor)\n",
    "            if neighbor_conflicts < next_conflicts:\n",
    "                next_state = neighbor\n",
    "                next_conflicts = neighbor_conflicts\n",
    "                break  # Move to the first better neighbor\n",
    "\n",
    "        # If no better neighbor is found, return the current state\n",
    "        if next_conflicts >= current_conflicts:\n",
    "            break\n",
    "\n",
    "        # Move to the better neighbor\n",
    "        current_state = next_state\n",
    "        current_conflicts = next_conflicts\n",
    "\n",
    "    return current_state, current_conflicts\n",
    "\n",
    "# Run Simple Hill Climbing for N-Queens\n",
    "n = 8  # Change N here for different sizes\n",
    "solution, conflicts = simple_hill_climbing(8)\n",
    "\n",
    "# Print results\n",
    "if conflicts == 0:\n",
    "    print(f\"Solution found for {n}-Queens problem:\")\n",
    "    print(solution)\n",
    "else:\n",
    "    print(f\"Could not find a solution. Stuck at state with {conflicts} conflicts:\")\n",
    "    print(solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a947dbf",
   "metadata": {},
   "source": [
    "### 8 Queens Problem using Best-choice hill-climbing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f483cc82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution: [2, 0, 7, 4, 1, 3, 0, 7], Conflicts: 2\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def calculate_attacks(state):\n",
    "    attacks = 0\n",
    "    n = len(state)\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            if state[i] == state[j] or abs(state[i] - state[j]) == abs(i - j):\n",
    "                attacks += 1\n",
    "    return attacks\n",
    "\n",
    "def get_best_neighbor(state):\n",
    "    n = len(state)\n",
    "    best_state = list(state)\n",
    "    min_attacks = calculate_attacks(state)\n",
    "\n",
    "    for row in range(n):\n",
    "        for col in range(n):\n",
    "            if col == state[row]:\n",
    "                continue\n",
    "            new_state = list(state)\n",
    "            new_state[row] = col\n",
    "            attacks = calculate_attacks(new_state)\n",
    "            if attacks < min_attacks:\n",
    "                best_state = new_state\n",
    "                min_attacks = attacks\n",
    "    return best_state, min_attacks\n",
    "\n",
    "def hill_climbing_queens():\n",
    "    state = [random.randint(0, 7) for _ in range(8)]\n",
    "    attacks = calculate_attacks(state)\n",
    "    while True:\n",
    "        neighbor, neighbor_attacks = get_best_neighbor(state)\n",
    "        if neighbor_attacks >= attacks:\n",
    "            return state, attacks\n",
    "        state, attacks = neighbor, neighbor_attacks\n",
    "\n",
    "solution, conflicts = hill_climbing_queens()\n",
    "print(f\"Solution: {solution}, Conflicts: {conflicts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650192ff",
   "metadata": {},
   "source": [
    "# Game Theory:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cacac86",
   "metadata": {},
   "source": [
    "### Simple MinMax Pseudocode:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc2d48b",
   "metadata": {},
   "source": [
    "function minimax(node, depth, maximizingPlayer) is\n",
    "    if depth == 0 or node is a terminal node then\n",
    "        return static evaluation of node\n",
    "\n",
    "    if MaximizingPlayer then // for Maximizer Player\n",
    "        maxEva = -infinity\n",
    "        for each child of node do\n",
    "            eva = minimax(child, depth - 1, false)\n",
    "            maxEva = max(maxEva, eva) // Gives the maximum of the values\n",
    "        return maxEva\n",
    "    else // for Minimizer Player\n",
    "        minEva = +infinity\n",
    "        for each child of node do\n",
    "            eva = minimax(child, depth - 1, true)\n",
    "            minEva = min(minEva, eva) // Gives the minimum of the values\n",
    "        return minEva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8a42fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed Nodes:  [2, 3, 'D', 5, 9, 'E', 'B', 0, 1, 'F', 7, 5, 'G', 'C', 'A']\n",
      "MinMax Values: \n",
      "A:  3\n",
      "B:  3\n",
      "C:  1\n",
      "D:  3\n",
      "E:  9\n",
      "F:  1\n",
      "G:  7\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "class Node:\n",
    "    def __init__(self, value=None):\n",
    "        self.value = value\n",
    "        self.children = []\n",
    "        self.minmax_value = None\n",
    "    \n",
    "class MinMaxAgent:\n",
    "    def __init__(self, depth):\n",
    "        self.depth = depth\n",
    "    def formulate_goal(self, node):\n",
    "        return \"Goal Reached\" if node.minmax_value is not None else \"Searchin\"\n",
    "    def act(self, node, environment):\n",
    "        goal_status = self.formulate_goal(node)\n",
    "        if goal_status == \"Goal Reached\":\n",
    "            return f\"Minmax valye for root node: {node.minmax_value}\"\n",
    "        else:\n",
    "            return environment.compute_minmax(node, self.depth)\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self, tree):\n",
    "        self.tree = tree\n",
    "        self.computed_nodes = []\n",
    "    def get_percept(self, node):\n",
    "        return node\n",
    "    def compute_minmax(self, node, depth, maximing_player=True):\n",
    "        if depth == 0 or not node.children:\n",
    "            self.computed_nodes.append(node.value)\n",
    "            return node.value\n",
    "        if maximing_player:\n",
    "            value = -math.inf\n",
    "            for child in node.children:\n",
    "                child_value = self.compute_minmax(child, depth -1, False)\n",
    "                value = max(value, child_value)\n",
    "            node.minmax_value = value\n",
    "            self.computed_nodes.append(node.value)\n",
    "            return value\n",
    "        else:\n",
    "            value = math.inf\n",
    "            for child in node.children:\n",
    "                child_value = self.compute_minmax(child, depth - 1, True)\n",
    "                value = min(value, child_value)\n",
    "            node.minmax_value = value\n",
    "            self.computed_nodes.append(node.value)\n",
    "            return value\n",
    "\n",
    "def run_agent(agent, environment, start_node):\n",
    "    percept = environment.get_percept(start_node)\n",
    "    agent.act(percept, environment)\n",
    "\n",
    "\n",
    "root = Node('A')\n",
    "n1 = Node('B')\n",
    "n2 = Node('C')\n",
    "root.children = [n1, n2]\n",
    "n3 = Node('D')\n",
    "n4 = Node('E')\n",
    "n5 = Node('F')\n",
    "n6 = Node('G')\n",
    "n1.children = [n3, n4]\n",
    "n2.children = [n5, n6]\n",
    "n7 = Node(2)\n",
    "n8 = Node(3)\n",
    "n9 = Node(5)\n",
    "n10 = Node(9)\n",
    "n3.children = [n7, n8]\n",
    "n4.children = [n9, n10]\n",
    "\n",
    "n11 = Node(0)\n",
    "n12 = Node(1)\n",
    "n13 = Node(7)\n",
    "n14 = Node(5)\n",
    "n5.children = [n11, n12]\n",
    "n6.children = [n13, n14]\n",
    "# define depth for Minimax\n",
    "depth = 3\n",
    "agent = MinMaxAgent(depth)\n",
    "environment = Environment(root)\n",
    "run_agent(agent, environment, root)\n",
    "print(\"Computed Nodes: \", environment.computed_nodes)\n",
    "\n",
    "print(\"MinMax Values: \")\n",
    "print(\"A: \", root.minmax_value)\n",
    "print(\"B: \", n1.minmax_value)\n",
    "print(\"C: \", n2.minmax_value)\n",
    "print(\"D: \", n3.minmax_value)\n",
    "print(\"E: \", n4.minmax_value)\n",
    "print(\"F: \", n5.minmax_value)\n",
    "print(\"G: \", n6.minmax_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0832ca8a",
   "metadata": {},
   "source": [
    "### Simple MinMax with Alpha-Beta Pruning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5df52e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned node: 5\n",
      "Pruned node: F\n",
      "Computed Nodes: ['A', 'B', 'D', 2, 3, 'E', 5, 'C', 'F', 0, 1]\n",
      "Minimax values:\n",
      "A: 3\n",
      "B: 3\n",
      "C: 1\n",
      "D: 3\n",
      "E: 5\n",
      "F: 1\n",
      "G: None\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, value=None):\n",
    "        self.value = value\n",
    "        self.children = []\n",
    "        self.minmax_value = None\n",
    "\n",
    "class MinimaxAgent:\n",
    "    def __init__(self, depth):\n",
    "        self.depth = depth\n",
    "\n",
    "    def formulate_goal(self, node):\n",
    "        return \"Goal reached\" if node.minmax_value is not None else \"Searching\"\n",
    "\n",
    "    def act(self, node, environment):\n",
    "        goal_status = self.formulate_goal(node)\n",
    "        if goal_status == \"Goal reached\":\n",
    "            return f\"Minimax value for root node: {node.minmax_value}\"\n",
    "        else:\n",
    "            return environment.alpha_beta_search(node, self.depth, -math.inf, math.inf, True)\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self, tree):\n",
    "        self.tree = tree\n",
    "        self.computed_nodes = []\n",
    "\n",
    "    def get_percept(self, node):\n",
    "        return node\n",
    "\n",
    "    def alpha_beta_search(self, node, depth, alpha, beta, maximizing_player=True):\n",
    "        self.computed_nodes.append(node.value)\n",
    "        if depth == 0 or not node.children:\n",
    "            return node.value\n",
    "        if maximizing_player:\n",
    "            value = -math.inf\n",
    "            for child in node.children:\n",
    "                value = max(value, self.alpha_beta_search(child, depth - 1, alpha, beta, False))\n",
    "                alpha = max(alpha, value)\n",
    "                if beta <= alpha:\n",
    "                    print(\"Pruned node:\", child.value)\n",
    "                    break\n",
    "            node.minmax_value = value\n",
    "            return value\n",
    "        else:\n",
    "            value = math.inf\n",
    "            for child in node.children:\n",
    "                value = min(value, self.alpha_beta_search(child, depth - 1, alpha, beta, True))\n",
    "                beta = min(beta, value)\n",
    "                if beta <= alpha:\n",
    "                    print(\"Pruned node:\", child.value)\n",
    "                    break\n",
    "            node.minmax_value = value\n",
    "            return value\n",
    "\n",
    "def run_agent(agent, environment, start_node):\n",
    "    percept = environment.get_percept(start_node)\n",
    "    agent.act(percept, environment)\n",
    "\n",
    "# Constructing the tree\n",
    "root = Node('A')\n",
    "n1 = Node('B')\n",
    "n2 = Node('C')\n",
    "root.children = [n1, n2]\n",
    "\n",
    "n3 = Node('D')\n",
    "n4 = Node('E')\n",
    "n5 = Node('F')\n",
    "n6 = Node('G')\n",
    "n1.children = [n3, n4]\n",
    "n2.children = [n5, n6]\n",
    "\n",
    "n7 = Node(2)\n",
    "n8 = Node(3)\n",
    "n9 = Node(5)\n",
    "n10 = Node(9)\n",
    "n3.children = [n7, n8]\n",
    "n4.children = [n9, n10]\n",
    "\n",
    "n11 = Node(0)\n",
    "n12 = Node(1)\n",
    "n13 = Node(7)\n",
    "n14 = Node(5)\n",
    "n5.children = [n11, n12]\n",
    "n6.children = [n13, n14]\n",
    "\n",
    "# Define depth for Alpha-Beta pruning\n",
    "depth = 3\n",
    "agent = MinimaxAgent(depth)\n",
    "environment = Environment(root)\n",
    "run_agent(agent, environment, root)\n",
    "\n",
    "print(\"Computed Nodes:\", environment.computed_nodes)\n",
    "print(\"Minimax values:\")\n",
    "print(f\"A: {root.minmax_value}\")\n",
    "print(f\"B: {n1.minmax_value}\")\n",
    "print(f\"C: {n2.minmax_value}\")\n",
    "print(f\"D: {n3.minmax_value}\")\n",
    "print(f\"E: {n4.minmax_value}\")\n",
    "print(f\"F: {n5.minmax_value}\")\n",
    "print(f\"G: {n6.minmax_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3240ebe",
   "metadata": {},
   "source": [
    "# Bayes' Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6ba105",
   "metadata": {},
   "source": [
    "### Burglary-Alarm System:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7cd19c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yesau\\AppData\\Roaming\\Python\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+\n",
      "| Burglary    |   phi(Burglary) |\n",
      "+=============+=================+\n",
      "| Burglary(0) |          0.9999 |\n",
      "+-------------+-----------------+\n",
      "| Burglary(1) |          0.0001 |\n",
      "+-------------+-----------------+\n"
     ]
    }
   ],
   "source": [
    "from pgmpy.models import DiscreteBayesianNetwork\n",
    "from pgmpy.factors.discrete import TabularCPD\n",
    "from pgmpy.inference import VariableElimination\n",
    "\n",
    "# Step 1: Define the structure of the Bayesian Network\n",
    "model = DiscreteBayesianNetwork([\n",
    "    ('Burglary', 'Alarm'),\n",
    "    ('Earthquake', 'Alarm'),\n",
    "    ('Alarm', 'JohnCalls'),\n",
    "    ('Alarm', 'MaryCalls')\n",
    "])\n",
    "\n",
    "# Step 2: Define the CPDs (Conditional Probability Distributions)\n",
    "cpd_burglary = TabularCPD(variable='Burglary', variable_card=2, values=[[0.999], [0.001]])\n",
    "cpd_earthquake = TabularCPD(variable='Earthquake', variable_card=2, values=[[0.998], [0.002]])\n",
    "cpd_alarm = TabularCPD(\n",
    "    variable='Alarm',\n",
    "    variable_card=2,\n",
    "    values=[\n",
    "        [0.999, 0.71, 0.06, 0.05],  # Alarm = False\n",
    "        [0.001, 0.29, 0.94, 0.95]   # Alarm = True\n",
    "    ],\n",
    "    evidence=['Burglary', 'Earthquake'],\n",
    "    evidence_card=[2, 2]\n",
    ")\n",
    "cpd_john = TabularCPD(\n",
    "    variable='JohnCalls',\n",
    "    variable_card=2,\n",
    "    values=[\n",
    "        [0.3, 0.9],  # JohnCalls = False\n",
    "        [0.7, 0.1]   # JohnCalls = True\n",
    "    ],\n",
    "    evidence=['Alarm'],\n",
    "    evidence_card=[2]\n",
    ")\n",
    "cpd_mary = TabularCPD(\n",
    "    variable='MaryCalls',\n",
    "    variable_card=2,\n",
    "    values=[\n",
    "        [0.2, 0.99],  # MaryCalls = False\n",
    "        [0.8, 0.01]   # MaryCalls = True\n",
    "    ],\n",
    "    evidence=['Alarm'],\n",
    "    evidence_card=[2]\n",
    ")\n",
    "\n",
    "# Step 3: Add CPDs to the model\n",
    "model.add_cpds(cpd_burglary, cpd_earthquake, cpd_alarm, cpd_john, cpd_mary)\n",
    "\n",
    "# Step 4: Verify the model\n",
    "assert model.check_model(), \"Model is incorrect\"\n",
    "\n",
    "# Step 5: Perform inference\n",
    "inference = VariableElimination(model)\n",
    "result = inference.query(variables=['Burglary'], evidence={'JohnCalls': 1, 'MaryCalls': 1})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b90d36",
   "metadata": {},
   "source": [
    "### Medical Diagnosis (Cancer Detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e117c09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+\n",
      "| Cancer    |   phi(Cancer) |\n",
      "+===========+===============+\n",
      "| Cancer(0) |        0.0500 |\n",
      "+-----------+---------------+\n",
      "| Cancer(1) |        0.9500 |\n",
      "+-----------+---------------+\n"
     ]
    }
   ],
   "source": [
    "from pgmpy.models import DiscreteBayesianNetwork\n",
    "from pgmpy.factors.discrete import TabularCPD\n",
    "from pgmpy.inference import VariableElimination\n",
    "\n",
    "model = DiscreteBayesianNetwork([('Pollution', 'Cancer'), ('Smoker', 'Cancer'), ('Cancer', 'XRay'), ('Cancer', 'Dyspnoea')])\n",
    "\n",
    "cpd_pollution = TabularCPD('Pollution', 2, [[0.9], [0.1]])\n",
    "cpd_smoker = TabularCPD('Smoker', 2, [[0.7], [0.3]])\n",
    "cpd_cancer = TabularCPD('Cancer', 2,\n",
    "    values=[[0.99, 0.9, 0.97, 0.05],\n",
    "            [0.01, 0.1, 0.03, 0.95]],\n",
    "    evidence=['Pollution', 'Smoker'],\n",
    "    evidence_card=[2, 2])\n",
    "cpd_xray = TabularCPD('XRay', 2, [[0.2, 0.9], [0.8, 0.1]], evidence=['Cancer'], evidence_card=[2])\n",
    "cpd_dyspnoea = TabularCPD('Dyspnoea', 2, [[0.65, 0.3], [0.35, 0.7]], evidence=['Cancer'], evidence_card=[2])\n",
    "\n",
    "model.add_cpds(cpd_pollution, cpd_smoker, cpd_cancer, cpd_xray, cpd_dyspnoea)\n",
    "assert model.check_model()\n",
    "\n",
    "inference = VariableElimination(model)\n",
    "result = inference.query(variables=['Cancer'], evidence={'Smoker': 1, 'Pollution': 1})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacb3888",
   "metadata": {},
   "source": [
    "### Spam Email Detection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cafeb971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+\n",
      "| Spam    |   phi(Spam) |\n",
      "+=========+=============+\n",
      "| Spam(0) |      0.3000 |\n",
      "+---------+-------------+\n",
      "| Spam(1) |      0.7000 |\n",
      "+---------+-------------+\n"
     ]
    }
   ],
   "source": [
    "model = DiscreteBayesianNetwork([('HasOffer', 'Spam'), ('ContainsLink', 'Spam')])\n",
    "\n",
    "cpd_offer = TabularCPD('HasOffer', 2, [[0.6], [0.4]])\n",
    "cpd_link = TabularCPD('ContainsLink', 2, [[0.7], [0.3]])\n",
    "cpd_spam = TabularCPD('Spam', 2,\n",
    "    values=[[0.95, 0.8, 0.85, 0.3],\n",
    "            [0.05, 0.2, 0.15, 0.7]],\n",
    "    evidence=['HasOffer', 'ContainsLink'],\n",
    "    evidence_card=[2, 2])\n",
    "\n",
    "model.add_cpds(cpd_offer, cpd_link, cpd_spam)\n",
    "assert model.check_model()\n",
    "\n",
    "inference = VariableElimination(model)\n",
    "result = inference.query(variables=['Spam'], evidence={'HasOffer': 1, 'ContainsLink': 1})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b185e7c7",
   "metadata": {},
   "source": [
    "### Student Exam Performance: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8dbcb6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+\n",
      "| Grade    |   phi(Grade) |\n",
      "+==========+==============+\n",
      "| Grade(0) |       0.7000 |\n",
      "+----------+--------------+\n",
      "| Grade(1) |       0.3000 |\n",
      "+----------+--------------+\n"
     ]
    }
   ],
   "source": [
    "model = DiscreteBayesianNetwork([('Study', 'Grade'), ('Sleep', 'Grade')])\n",
    "\n",
    "cpd_study = TabularCPD('Study', 2, [[0.3], [0.7]])\n",
    "cpd_sleep = TabularCPD('Sleep', 2, [[0.4], [0.6]])\n",
    "cpd_grade = TabularCPD('Grade', 2,\n",
    "    values=[[0.9, 0.6, 0.7, 0.1],\n",
    "            [0.1, 0.4, 0.3, 0.9]],\n",
    "    evidence=['Study', 'Sleep'],\n",
    "    evidence_card=[2, 2])\n",
    "\n",
    "model.add_cpds(cpd_study, cpd_sleep, cpd_grade)\n",
    "assert model.check_model()\n",
    "\n",
    "inference = VariableElimination(model)\n",
    "result = inference.query(variables=['Grade'], evidence={'Study': 1, 'Sleep': 0})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ba6e60",
   "metadata": {},
   "source": [
    "## EDA + Model Training + Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fabda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# ---------------------------------------\n",
    "# 1. Load and Prepare the Data\n",
    "# ---------------------------------------\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "df['species'] = iris.target\n",
    "df['species'] = df['species'].apply(lambda x: iris.target_names[x])  # Convert to labels\n",
    "\n",
    "# Show first few rows\n",
    "print(\"Sample data:\")\n",
    "print(df.head())\n",
    "\n",
    "# ---------------------------------------\n",
    "# 2. Exploratory Data Analysis (EDA)\n",
    "# ---------------------------------------\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Pairplot for feature relationships\n",
    "sns.pairplot(df, hue='species')\n",
    "plt.suptitle(\"Pairwise Feature Relationships\", y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Correlation heatmap\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.heatmap(df.drop('species', axis=1).corr(), annot=True, cmap='coolwarm')\n",
    "plt.title(\"Feature Correlation Heatmap\")\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------------------\n",
    "# 3. Model Training\n",
    "# ---------------------------------------\n",
    "\n",
    "# Encode species as integers\n",
    "df['species'] = df['species'].astype('category').cat.codes\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('species', axis=1)\n",
    "y = df['species']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the logistic regression model\n",
    "model = LogisticRegression(max_iter=200)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ---------------------------------------\n",
    "# 4. Model Evaluation\n",
    "# ---------------------------------------\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Accuracy score\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nAccuracy on test set: {acc:.2f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=iris.target_names,\n",
    "            yticklabels=iris.target_names)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccebc90",
   "metadata": {},
   "source": [
    "#### Reading and Writing Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d37936",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"Data.csv\") # Reading a CSV file\n",
    "df.to_csv(\"output.csv\", index=False) # Writing to a CSV file\n",
    "print(df.head()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3cd06c",
   "metadata": {},
   "source": [
    "#### Handling missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111124e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(df.isnull().sum())\n",
    "# Drop missing values\n",
    "df_cleaned = df.dropna()\n",
    "print(\"Dataset after dropping missing values:\", df_cleaned.shape)\n",
    "# Handle missing values\n",
    "df['age'] = df['age'].fillna(df['age'].mean()) # Fill age with mean\n",
    "df['embarked'] = df['embarked'].fillna(df['embarked'].mode()[0]) # Fill embarked with mode\n",
    "# Convert deck to string and replace NaN\n",
    "df['deck'] = df['deck'].astype(str).fillna('Unknown')\n",
    "# Drop remaining NaN values if needed\n",
    "df = df.dropna()\n",
    "print(df.isnull().sum()) # Confirm no missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8d3ce7",
   "metadata": {},
   "source": [
    "Handling missing data\n",
    "\n",
    "Label encoding and one-hot encoding\n",
    "\n",
    "Data visualization\n",
    "\n",
    "Train-test split\n",
    "\n",
    "K-Fold cross-validation\n",
    "\n",
    "Leave-One-Out cross-validation\n",
    "\n",
    "Confusion matrix\n",
    "\n",
    "ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4cfb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, LeaveOneOut\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "\n",
    "# Load a sample dataset (Breast Cancer dataset)\n",
    "data = load_breast_cancer()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target\n",
    "\n",
    "# Introduce some missing values for demonstration\n",
    "df.iloc[0:10, 0] = np.nan\n",
    "\n",
    "# Handling missing data using SimpleImputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df[df.columns[:-1]] = imputer.fit_transform(df[df.columns[:-1]])\n",
    "\n",
    "# Label encoding (not needed here since target is already numeric)\n",
    "# Just a demonstration\n",
    "label_encoder = LabelEncoder()\n",
    "df['target'] = label_encoder.fit_transform(df['target'])\n",
    "\n",
    "# One-Hot Encoding (if we had categorical features)\n",
    "# Here we create a dummy feature to simulate\n",
    "df['dummy_cat'] = np.random.choice(['A', 'B', 'C'], size=len(df))\n",
    "df = pd.get_dummies(df, columns=['dummy_cat'], drop_first=True)\n",
    "\n",
    "# Data visualization\n",
    "sns.countplot(x='target', data=df)\n",
    "plt.title(\"Target Class Distribution\")\n",
    "plt.show()\n",
    "\n",
    "# Correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(df.corr(), cmap='coolwarm', annot=False)\n",
    "plt.title(\"Feature Correlation Heatmap\")\n",
    "plt.show()\n",
    "\n",
    "# Train-test split\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Model training using Logistic Regression\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate on test set\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# ROC Curve\n",
    "y_prob = model.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='red', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "kfold_scores = cross_val_score(model, X, y, cv=kfold)\n",
    "print(\"K-Fold CV Scores:\", kfold_scores)\n",
    "print(\"Average K-Fold Score:\", np.mean(kfold_scores))\n",
    "\n",
    "# Leave-One-Out Cross Validation\n",
    "loo = LeaveOneOut()\n",
    "loo_scores = cross_val_score(model, X, y, cv=loo)\n",
    "print(\"Leave-One-Out CV Accuracy (mean):\", np.mean(loo_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2da4393",
   "metadata": {},
   "source": [
    "### Support Vector Machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "659472ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Load dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "y = (y == 0).astype(int) # Convert to binary classification problem\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n",
    "random_state=42)\n",
    "# Train SVM model with RBF kernel\n",
    "svm = SVC(kernel='rbf', C=1, gamma='scale')\n",
    "svm.fit(X_train, y_train)\n",
    "# Make predictions\n",
    "y_pred = svm.predict(X_test)\n",
    "# Evaluate the model\n",
    "print(\"SVM Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c3bff0",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "79f2c9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sklearn.datasets._california_housing:Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to C:\\Users\\yesau\\scikit_learn_data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on test data:\n",
      " [0.71912284 1.76401657 2.70965883 ... 4.46877017 1.18751119 2.00940251]\n",
      "\n",
      "Mean Squared Error: 0.56\n",
      "R² Score: 0.58\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load a sample dataset\n",
    "data = fetch_california_housing()\n",
    "x = pd.DataFrame(data.data, columns=data.feature_names)  # Features\n",
    "y = pd.Series(data.target)  # Target (house values)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the Linear Regression model\n",
    "LR = LinearRegression()\n",
    "ModelLR = LR.fit(x_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "PredictionLR = ModelLR.predict(x_test)\n",
    "\n",
    "# Print the predictions\n",
    "print(\"Predictions on test data:\\n\", PredictionLR)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, PredictionLR)\n",
    "r2 = r2_score(y_test, PredictionLR)\n",
    "\n",
    "print(f\"\\nMean Squared Error: {mse:.2f}\")\n",
    "print(f\"R² Score: {r2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2afe406",
   "metadata": {},
   "source": [
    "### Decision tree classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4530e8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
      "==================== DT Training Accuracy ====================\n",
      "Training Accuracy: 100.00%\n",
      "==================== DT Testing Accuracy =====================\n",
      "Testing Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load sample dataset\n",
    "data = load_iris()\n",
    "x = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target)\n",
    "\n",
    "# Split the dataset\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the DecisionTreeClassifier\n",
    "DT = DecisionTreeClassifier()\n",
    "\n",
    "# Train the model\n",
    "ModelDT = DT.fit(x_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "PredictionDT = DT.predict(x_test)\n",
    "print(\"Predictions:\", PredictionDT)\n",
    "\n",
    "# Model Training Accuracy\n",
    "print('==================== DT Training Accuracy ====================')\n",
    "tracDT = DT.score(x_train, y_train)  # The score method gives accuracy directly\n",
    "TrainingAccDT = tracDT * 100\n",
    "print(f\"Training Accuracy: {TrainingAccDT:.2f}%\")\n",
    "\n",
    "# Model Testing Accuracy\n",
    "print('==================== DT Testing Accuracy =====================')\n",
    "teacDT = accuracy_score(y_test, PredictionDT)\n",
    "testingAccDT = teacDT * 100\n",
    "print(f\"Testing Accuracy: {testingAccDT:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba881c4",
   "metadata": {},
   "source": [
    "### K-Means clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a10a06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Importing the dataset\n",
    "df = pd.read_csv('Mall_Customers.csv')\n",
    "\n",
    "# Extracting relevant features: Annual Income and Spending Score\n",
    "x = df.iloc[:, [3, 4]].values\n",
    "\n",
    "# Finding optimal number of clusters using the Elbow Method\n",
    "wcss_list = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)\n",
    "    kmeans.fit(x)\n",
    "    wcss_list.append(kmeans.inertia_)\n",
    "\n",
    "# Plotting the Elbow Graph\n",
    "plt.plot(range(1, 11), wcss_list)\n",
    "plt.title('The Elbow Method Graph')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n",
    "\n",
    "# Normalize features for better clustering\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(x)\n",
    "\n",
    "# Training the KMeans model on the dataset\n",
    "kmeans = KMeans(n_clusters=5, init='k-means++', random_state=42)\n",
    "y_predict = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Visualizing the clusters\n",
    "plt.scatter(x[y_predict == 0, 0], x[y_predict == 0, 1], s=100, c='blue', label='Cluster 1')\n",
    "plt.scatter(x[y_predict == 1, 0], x[y_predict == 1, 1], s=100, c='green', label='Cluster 2')\n",
    "plt.scatter(x[y_predict == 2, 0], x[y_predict == 2, 1], s=100, c='red', label='Cluster 3')\n",
    "plt.scatter(x[y_predict == 3, 0], x[y_predict == 3, 1], s=100, c='black', label='Cluster 4')\n",
    "plt.scatter(x[y_predict == 4, 0], x[y_predict == 4, 1], s=100, c='purple', label='Cluster 5')\n",
    "\n",
    "# Plotting Centroids\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], \n",
    "            s=300, c='yellow', label='Centroid')\n",
    "\n",
    "plt.title('Clusters of Customers')\n",
    "plt.xlabel('Annual Income (k$)')\n",
    "plt.ylabel('Spending Score (1-100)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06da9334",
   "metadata": {},
   "source": [
    "### Decision Tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "729b9dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [ 0.06858754  1.00867146 -0.44710026  0.11520442  0.06738308  0.9268164\n",
      "  0.30149058  0.03285725 -0.05930489  0.19989713  0.05860855  0.83360058\n",
      "  1.01238455  0.84301552  1.02396928  0.1851498  -0.15568455  0.14419308\n",
      "  0.08710758 -0.16992588  0.83271001  0.04251129  0.88704775 -0.16397104\n",
      "  0.00190482 -0.02397524 -0.25389267 -0.11680648  0.82169091  0.81105763]\n",
      "===================LR Testing Accuracy================\n",
      "88.73227699320645\n",
      "Predictions:  [0 1 0 0 0 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1]\n",
      "====================DT Training Accuracy===============\n",
      "Training Accuracy: 100.00%\n",
      "=====================DT Testing Accuracy=================\n",
      "Testing Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from sklearn import datasets\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Load dataset\n",
    "iris = datasets.load_iris()\n",
    "x = iris.data\n",
    "y = iris.target\n",
    "y = (y == 0).astype(int)\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2,\n",
    "random_state=42)\n",
    "# Create and train the Linear Regression model\n",
    "LR = LinearRegression()\n",
    "ModelLR = LR.fit(x_train, y_train)\n",
    "# Predict on the test data\n",
    "PredictionLR = ModelLR.predict(x_test)\n",
    "# Print the predictions\n",
    "print(\"Predictions:\", PredictionLR)\n",
    "from sklearn.metrics import r2_score\n",
    "print('===================LR Testing Accuracy================')\n",
    "teachLR = r2_score(y_test, PredictionLR)\n",
    "testingAccLR = teachLR * 100\n",
    "print(testingAccLR)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "DT = DecisionTreeClassifier()\n",
    "ModelDt = DT.fit(x_train, y_train)\n",
    "\n",
    "PredictionDT = DT.predict(x_test)\n",
    "print(\"Predictions: \", PredictionDT)\n",
    "\n",
    "print('====================DT Training Accuracy===============')\n",
    "tracDT = DT.score(x_train, y_train)\n",
    "TrainingAccDT = tracDT * 100\n",
    "print(f\"Training Accuracy: {TrainingAccDT:.2f}%\")\n",
    "\n",
    "# Model Testing Accuracy\n",
    "print('=====================DT Testing Accuracy=================')\n",
    "teacDT = accuracy_score(y_test, PredictionDT)\n",
    "testingAccDT = teacDT * 100\n",
    "print(f\"Testing Accuracy: {testingAccDT:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
